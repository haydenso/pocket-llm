# making llm inference in the browser go brrrrr

A super lightweight web application that runs large language models directly in your browser using WebGPU acceleration.

todos: add browser detection and to make sure the model can run.

