## Features

- **100% Browser-based**: No server required, everything runs locally
- **WebGPU Acceleration**: Leverages your GPU for fast inference
- **Streaming Responses**: Real-time token-by-token generation
- **Privacy-First**: All processing happens in your browser
- **Lightweight**: Minimal dependencies and clean code

## Tech Stack

- **WebLLM**: High-performance in-browser LLM inference engine
- **Vite**: Fast build tool and dev server
- **Vanilla JS**: No framework overhead
- **WebGPU**: Hardware acceleration

## Model

Currently using **Qwen2.5-0.5B-Instruct** (~500MB) for fast loading and inference.

### Switching Models

You can easily switch to other models by changing the `selectedModel` in `main.js`:

```javascript
// Available small models:
const selectedModel = "Qwen2.5-0.5B-Instruct-q4f16_1-MLC";  // ~500MB
// const selectedModel = "Phi-3.5-mini-instruct-q4f16_1-MLC";  // ~2GB
// const selectedModel = "Llama-3.2-1B-Instruct-q4f16_1-MLC";  // ~1GB
```

For the complete list of supported models, visit: https://mlc.ai/models

## Requirements

- Modern browser with WebGPU support (Chrome 113+, Edge 113+)
- At least 2GB RAM available
- Decent GPU (integrated GPU works for small models)

## Getting Started

1. **Install dependencies:**

   ```bash
   npm install
   ```
2. **Start development server:**

   ```bash
   npm run dev
   ```
3. **Open in browser:**
   Navigate to `http://localhost:5173`
4. **Wait for model to load:**
   First run will download the model (~500MB). Subsequent loads will use browser cache.

## Building for Production

```bash
npm run build
npm run preview
```

## Project Structure

```
trinity-web/
├── index.html       # Main HTML with embedded styles
├── main.js          # WebLLM integration and chat logic
├── vite.config.js   # Vite config with required headers
├── package.json     # Dependencies
└── README.md        # This file
```

## How It Works

1. **Model Loading**: WebLLM downloads and caches the model in browser storage
2. **WebGPU Initialization**: Creates WebGPU compute pipelines for model inference
3. **Chat Interface**: Streams responses token-by-token for real-time interaction
4. **State Management**: Maintains conversation history for context-aware responses

## Browser Compatibility

| Browser     | Support                            |
| ----------- | ---------------------------------- |
| Chrome 113+ | ✅ Full support                    |
| Edge 113+   | ✅ Full support                    |
| Safari      | ⚠️ Limited (WebGPU experimental) |
| Firefox     | ❌ Not yet (WebGPU in development) |

## Performance Tips

- Use a smaller model (0.5B-1B parameters) for faster inference
- Close other tabs to free up GPU memory
- First load takes time - be patient!
- Subsequent loads are much faster (cached)

## Troubleshooting

**"WebGPU is not supported"**

- Update your browser to the latest version
- Check if your GPU supports WebGPU

**Model loading stuck**

- Check your internet connection
- Clear browser cache and try again
- Check browser console for errors

**Slow inference**

- Try a smaller model
- Close other GPU-intensive applications
- Ensure you're not on battery saver mode

## License

MIT

## Acknowledgments

- [WebLLM](https://github.com/mlc-ai/web-llm) by MLC-AI
- [Trinity-Nano](https://huggingface.co/arcee-ai/Trinity-Nano-Preview-GGUF) by Arcee AI
- WebGPU community
